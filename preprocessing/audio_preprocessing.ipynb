{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e962e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\egorv\\Desktop\\BProj\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal as signal\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import sounddevice as sd\n",
    "from pathlib import Path\n",
    "import pandas\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model, HubertModel\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import gc\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#c:/Users/egorv/Desktop/BProj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966d0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASVSpoofDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, flac_dir, labels_path):\n",
    "        \"\"\"\n",
    "        Returns all the directory where the flac_files are located, returns the files itself,\n",
    "        returns the dataset with filenames, targets, speaker ID, and type attack ID.\n",
    "        Also returns the list of filenames, and target dictionary\n",
    "        \"\"\"\n",
    "        self.flac_dir = flac_dir\n",
    "        self.files = sorted(Path(flac_dir).glob(\"*.flac\"))\n",
    "        self.labels_df = pandas.read_csv(labels_path, sep=r\"\\s+\", header=None)\n",
    "        self.file_names = self.labels_df[1]\n",
    "        self.target = dict(zip(self.labels_df[1], self.labels_df[4]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        file_path = self.files[idx]\n",
    "  \n",
    "        audio, sr = sf.read(str(file_path), dtype=\"float32\", always_2d=True)\n",
    "        x = torch.from_numpy(audio.T)\n",
    "        x = x.mean(dim=0)     \n",
    " \n",
    "        x = self.normalize_duration(x)\n",
    "        file_name = file_path.stem\n",
    "        target_str = self.target.get(file_name)\n",
    "\n",
    "        y = 1 if target_str == 'bonafide' else 0\n",
    "        return x, torch.tensor(y).long()\n",
    "\n",
    "    def normalize_duration(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor формы (samples,) или (1, samples)\n",
    "        \"\"\"\n",
    "        TARGET_SEC = 4.0\n",
    "        TARGET_LEN = int(16000 * TARGET_SEC)\n",
    "        \n",
    "        if x.ndim > 1:\n",
    "            x = x.squeeze()\n",
    "\n",
    "        cur_len = x.shape[0]\n",
    "\n",
    "        if cur_len > TARGET_LEN:\n",
    "            start = torch.randint(0, cur_len - TARGET_LEN + 1, (1,)).item()\n",
    "            return x[start : start + TARGET_LEN]\n",
    "        \n",
    "        elif cur_len < TARGET_LEN:\n",
    "            pad_len = TARGET_LEN - cur_len\n",
    "            return torch.nn.functional.pad(x, (0, pad_len), mode='constant', value=0)\n",
    "        \n",
    "        return x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.hubert = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "        combined_dim = self.wav2vec2.config.hidden_size + self.hubert.config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(combined_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w2v_out = self.wav2vec2(x).last_hidden_state.mean(dim=1)\n",
    "        hubert_out = self.hubert(x).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        combined = torch.cat((w2v_out, hubert_out), dim=1)\n",
    "\n",
    "        return self.classifier(combined)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df704ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, train_loader, optimizer, loss_fn, model, device):\n",
    "    print(\"Entered train_one_epoch\")\n",
    "    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (audios, labels) in enumerate(train_loader):\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\"first batch ok\", audios.shape, labels.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        audios, labels = audios.to(device), labels.to(device)\n",
    "        outputs = model(audios)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        total_loss += loss_val\n",
    "        batch_loss += loss_val\n",
    "        tb_writer.add_scalar(\"Loss/Train_batch\", batch_loss, batch_idx + 1)\n",
    "        print(f\"Batch {batch_idx + 1} loss: {batch_loss}\")\n",
    "\n",
    "    epoch_avg = total_loss / max(1, len(train_loader))\n",
    "    tb_writer.add_scalar(\"Loss/Train_epoch\", epoch_avg, epoch_index + 1)\n",
    "    tb_writer.flush()\n",
    "    return epoch_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d1f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"code\" / \"experiments\" / \"data\" / \"ASVSpoof2019\"\n",
    "\n",
    "train_flac_dir = (DATA_DIR/ \"LA\" / \"ASVSpoof2019_LA_train\" / \"flac\")\n",
    "labels_file = DATA_DIR / \"LA\" / \"ASVspoof2019_LA_cm_protocols\" / \"ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "\n",
    "flac_dataset = ASVSpoofDataset(train_flac_dir, labels_file)\n",
    "    \n",
    "\n",
    "indices = list(range(len(flac_dataset)))\n",
    "\n",
    "train_split, temp_split = train_test_split(indices, train_size=0.8, shuffle=True, random_state=10)\n",
    "val_split, test_split = train_test_split(temp_split, train_size=0.5, random_state=10)\n",
    "\n",
    "train_subset = Subset(flac_dataset, train_split)\n",
    "val_subset = Subset(flac_dataset, val_split)\n",
    "test_subset = Subset(flac_dataset, test_split)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=16, num_workers=8, persistent_workers=True, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=16, num_workers=8, shuffle=False)\n",
    "test_loader = DataLoader(test_subset, batch_size=16, num_workers=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# persistent_workers=True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EnsembleModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "num_epochs = 5\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "project_root = Path(r\"C:\\Users\\egorv\\Desktop\\BProj\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "log_dir = project_root / \"runs\" / f\"fashion_trainer_{timestamp}\"\n",
    "writer = SummaryWriter(str(log_dir))\n",
    "\n",
    "print(\"Writing logs to:\", log_dir)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        \n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch, writer, train_loader, optimizer, criterion, model, device)\n",
    "    avg_loss_f = float(avg_loss)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "    avg_vloss = running_vloss / (i+1)\n",
    "    avg_vloss_f = float(avg_vloss)\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss_f, 'Validation' : avg_vloss_f },\n",
    "                        epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = project_root / f\"model_{timestamp}_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), str(model_path))\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64100074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 211/211 [00:00<00:00, 1073.04it/s, Materializing param=masked_spec_embed]                                            \n",
      "Wav2Vec2Model LOAD REPORT from: facebook/wav2vec2-base\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "project_q.weight             | UNEXPECTED |  | \n",
      "project_q.bias               | UNEXPECTED |  | \n",
      "quantizer.codevectors        | UNEXPECTED |  | \n",
      "quantizer.weight_proj.weight | UNEXPECTED |  | \n",
      "project_hid.weight           | UNEXPECTED |  | \n",
      "quantizer.weight_proj.bias   | UNEXPECTED |  | \n",
      "project_hid.bias             | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 211/211 [00:00<00:00, 1040.49it/s, Materializing param=masked_spec_embed]                                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = Path(r\"C:\\Users\\egorv\\Desktop\\BProj\")\n",
    "   \n",
    "model_path = project_root / \"model_20260129_011815_3.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EnsembleModel().to(device)\n",
    "state = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92b5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "Test accuracy: 0.9992119779353822\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audios, labels in loader:\n",
    "            print(\"done\")\n",
    "            audios, labels = audios.to(device), labels.to(device)\n",
    "            logits = model(audios)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "\n",
    "    return correct / max(1, total)\n",
    "\n",
    "acc = evaluate(model, test_loader, device)\n",
    "print(\"Test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c567d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flac_dir = (DATA_DIR/ \"LA\" / \"ASVSpoof2019_LA_eval\" / \"flac\")\n",
    "test_file = DATA_DIR / \"LA\" / \"ASVspoof2019_LA_cm_protocols\" / \"ASVspoof2019.LA.cm.eval.trl.txt\"\n",
    "\n",
    "test_dataset = ASVSpoofDataset(test_flac_dir, test_file)\n",
    "test_loader = DataLoader(test_subset, batch_size=16, num_workers=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf5d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.0013111888111888112\n",
      "Best threshold: 0.020005112513899803\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def collect_scores(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audios, labels in loader:\n",
    "            audios = audios.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(audios)              # (B,2)\n",
    "            probs = F.softmax(logits, dim=1)    # (B,2)\n",
    "            score_bonafide = probs[:, 1]        # вероятность класса 1 (bonafide)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy().tolist())\n",
    "            y_score.extend(score_bonafide.cpu().numpy().tolist())\n",
    "\n",
    "    return np.array(y_true), np.array(y_score)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def compute_eer_sklearn(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # ищем точку где |FPR - FNR| минимальна\n",
    "    idx = np.nanargmin(np.abs(fpr - fnr))\n",
    "    eer = (fpr[idx] + fnr[idx]) / 2.0\n",
    "    thr = thresholds[idx]\n",
    "    return eer, thr\n",
    "\n",
    "y_true, y_score = collect_scores(model, test_loader, device)\n",
    "eer, thr = compute_eer_sklearn(y_true, y_score)\n",
    "\n",
    "print(\"EER:\", eer)\n",
    "print(\"Best threshold:\", thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64ce1916",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     right = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m a \u001b[38;5;28;01mif\u001b[39;00m x > pivot]\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m quicksort(left) + mid + quicksort(right)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m a = \u001b[43mquicksort\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mquicksort\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[32m      8\u001b[39m pivot = a[\u001b[38;5;28mlen\u001b[39m(a) // \u001b[32m2\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m left = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m a \u001b[38;5;28;01mif\u001b[39;00m x < pivot]\n\u001b[32m     10\u001b[39m mid = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m a \u001b[38;5;28;01mif\u001b[39;00m x == pivot]\n\u001b[32m     11\u001b[39m right = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m a \u001b[38;5;28;01mif\u001b[39;00m x > pivot]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randint(0, 100, size=1_000_000_000)\n",
    "\n",
    "def quicksort(a):\n",
    "    if len(a) <= 1:\n",
    "        return a\n",
    "    pivot = a[len(a) // 2]\n",
    "    left = [x for x in a if x < pivot]\n",
    "    mid = [x for x in a if x == pivot]\n",
    "    right = [x for x in a if x > pivot]\n",
    "    return quicksort(left) + mid + quicksort(right)\n",
    "\n",
    "a = quicksort(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e766c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0, 100, size=1_000_000_000)\n",
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02095de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "[2, 4, 6, 9, 10, 9, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771d258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
