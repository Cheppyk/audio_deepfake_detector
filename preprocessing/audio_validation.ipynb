{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ec41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model, HubertModel\n",
    "import pandas\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"code\" / \"experiments\" / \"data\" / \"ASVSpoof2019\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d4b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.hubert = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "        combined_dim = self.wav2vec2.config.hidden_size + self.hubert.config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(combined_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w2v_out = self.wav2vec2(x).last_hidden_state.mean(dim=1)\n",
    "        hubert_out = self.hubert(x).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        combined = torch.cat((w2v_out, hubert_out), dim=1)\n",
    "\n",
    "        return self.classifier(combined)\n",
    "    \n",
    "class ASVSpoofDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, flac_dir, labels_path):\n",
    "        \"\"\"\n",
    "        Returns all the directory where the flac_files are located, returns the files itself,\n",
    "        returns the dataset with filenames, targets, speaker ID, and type attack ID.\n",
    "        Also returns the list of filenames, and target dictionary\n",
    "        \"\"\"\n",
    "        self.flac_dir = flac_dir\n",
    "        self.files = sorted(Path(flac_dir).glob(\"*.flac\"))\n",
    "        self.labels_df = pandas.read_csv(labels_path, sep=r\"\\s+\", header=None)\n",
    "        self.file_names = self.labels_df[1]\n",
    "        self.target = dict(zip(self.labels_df[1], self.labels_df[4]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        file_path = self.files[idx]\n",
    "  \n",
    "        audio, sr = sf.read(str(file_path), dtype=\"float32\", always_2d=True)\n",
    "        x = torch.from_numpy(audio.T)\n",
    "        x = x.mean(dim=0)     \n",
    " \n",
    "        x = self.normalize_duration(x)\n",
    "        file_name = file_path.stem\n",
    "        target_str = self.target.get(file_name)\n",
    "\n",
    "        y = 1 if target_str == 'bonafide' else 0\n",
    "        return x, torch.tensor(y).long()\n",
    "\n",
    "    def normalize_duration(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor формы (samples,) или (1, samples)\n",
    "        \"\"\"\n",
    "        TARGET_SEC = 4.0\n",
    "        TARGET_LEN = int(16000 * TARGET_SEC)\n",
    "        \n",
    "        if x.ndim > 1:\n",
    "            x = x.squeeze()\n",
    "\n",
    "        cur_len = x.shape[0]\n",
    "\n",
    "        if cur_len > TARGET_LEN:\n",
    "            start = torch.randint(0, cur_len - TARGET_LEN + 1, (1,)).item()\n",
    "            return x[start : start + TARGET_LEN]\n",
    "        \n",
    "        elif cur_len < TARGET_LEN:\n",
    "            pad_len = TARGET_LEN - cur_len\n",
    "            return torch.nn.functional.pad(x, (0, pad_len), mode='constant', value=0)\n",
    "        \n",
    "        return x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6057f503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 211/211 [00:00<00:00, 1200.66it/s, Materializing param=masked_spec_embed]                                            \n",
      "Wav2Vec2Model LOAD REPORT from: facebook/wav2vec2-base\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "quantizer.weight_proj.bias   | UNEXPECTED |  | \n",
      "quantizer.weight_proj.weight | UNEXPECTED |  | \n",
      "project_q.bias               | UNEXPECTED |  | \n",
      "project_hid.weight           | UNEXPECTED |  | \n",
      "quantizer.codevectors        | UNEXPECTED |  | \n",
      "project_q.weight             | UNEXPECTED |  | \n",
      "project_hid.bias             | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 211/211 [00:00<00:00, 903.97it/s, Materializing param=masked_spec_embed]                                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = Path(r\"C:\\Users\\egorv\\Desktop\\BProj\")\n",
    "   \n",
    "model_path = project_root / \"model_20260129_011815_3.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EnsembleModel().to(device)\n",
    "state = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flac_dir = (DATA_DIR/ \"LA\" / \"ASVSpoof2019_LA_eval\" / \"flac\")\n",
    "test_file = DATA_DIR / \"LA\" / \"ASVspoof2019_LA_cm_protocols\" / \"ASVspoof2019.LA.cm.eval.trl.txt\"\n",
    "\n",
    "test_dataset = ASVSpoofDataset(test_flac_dir, test_file)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "def collect_scores(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audios, labels in loader:\n",
    "            audios = audios.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(audios)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            score_bonafide = probs[:, 1]\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy().tolist())\n",
    "            y_score.extend(score_bonafide.cpu().numpy().tolist())\n",
    "\n",
    "    return np.array(y_true), np.array(y_score)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def compute_eer_sklearn(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    idx = np.nanargmin(np.abs(fpr - fnr))\n",
    "    eer = (fpr[idx] + fnr[idx]) / 2.0\n",
    "    thr = thresholds[idx]\n",
    "    return eer, thr\n",
    "\n",
    "y_true, y_score = collect_scores(model, test_loader, device)\n",
    "eer, thr = compute_eer_sklearn(y_true, y_score)\n",
    "\n",
    "print(\"EER:\", eer)\n",
    "print(\"Best threshold:\", thr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
